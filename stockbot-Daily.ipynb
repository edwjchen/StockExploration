{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cogs 109: Modeling and Data Analysis\n",
    "## Final project guidelines, 2019\n",
    "\n",
    "Work in teams of at least 2 and no more than 4 students. Every student in the group will be expected to contribute substantially to the final product(s), and all students should be able to understand and explain all aspects of the project when you present your work in the final symposium.\n",
    "\n",
    "Your project should. \n",
    "- Identify a real problem, challenge or scientific question which could benefit from data analysis and modeling. Your final report must explain why the question is interesting or important. \n",
    "- Identify a relevant data set. You should learn about how the data was collected and be able to explain key features of the data, for example: How many observations? What are the noise sources? What are the relevant predictors?\n",
    "Identify at least one relevant data analysis approach, choosing from the methods covered in the course (linear or nonlinear regression, classification, clustering, PCA, etc.). Explain why this analysis approach is appropriate for addressing your question.\n",
    "- Identify and explain one or more hypotheses or initial expectation that you will test using the data.\n",
    "- Model selection: You should compare and contrast multiple different models (at least 2, but usually more). Your comparison should make use of cross-validation, bootstrap sampling, regularization, and/or other relevant techniques. For example, you might compare K-Nearest Neighbors classification for a range of k values (k=1,2,…,50), and select the k value that provides the lowest test set (cross-validation) error.\n",
    "- Model estimation: Implement your data analysis and present the results using a combination of data visualizations (box plots, scatter plots), statistical analyses and models.\n",
    "- Present your conclusions and outlook for next steps/future directions.\n",
    "\n",
    "The final product will be a written report, 5-10 pages in length. In addition, you will create a poster explaining your project to be presented in a symposium session on the last day of class. We will provide more information about the final paper and poster in a few weeks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written report:\n",
    "Your final report must include the following sections (use these headings).\n",
    "- Introduction. \n",
    "    - Define the real problem and explain its motivation\n",
    "    - Identify the dataset you will use and explain its key characteristics.\n",
    "    - Explain at least one hypothesis that you will test.\n",
    "- Methods. Identify the data analysis approach you will use and explain the rationale/motivation for your choice of this approach.\n",
    "- Results\n",
    "    - Model selection. You MUST compare at least 2 models, using cross-validation, regularization, and/or other relevant techniques.\n",
    "    - Model estimation. What are the final parameter estimates? What is the final accuracy of the model’s predictions?\n",
    "    - Conclusions and discussion. What can you conclude about your hypothesis? (Note that negative or ambiguous results are perfectly acceptable, you just need to explain what you found.) What are some potential implications/next steps for researchers interested in this topic?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "from scipy import linalg\n",
    "from sklearn.linear_model import Ridge\n",
    "from pytrends.request import TrendReq\n",
    "import os.path\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://stackoverflow.com/questions/50786266/writing-dictionary-of-dataframes-to-file\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def save_dict_df(dictex, keys_filename=\"keys.txt\", data_folder=\"stock_data\"):\n",
    "    for key, val in dictex.items():\n",
    "        val.to_csv(\"./\"+data_folder+\"/data_{}.csv\".format(str(key)))\n",
    "\n",
    "    with open(keys_filename, \"w\") as f: #saving keys to file\n",
    "        f.write(str(list(dictex.keys())))\n",
    "\n",
    "def load_dict_df(keys_filename=\"keys.txt\", data_folder=\"stock_data\"):\n",
    "    \"\"\"Reading data from keys\"\"\"\n",
    "    with open(keys_filename, \"r\") as f:\n",
    "        keys = eval(f.read())\n",
    "\n",
    "    dictex = {}    \n",
    "    for key in keys:\n",
    "        dictex[key] = pd.read_csv(\"./\"+data_folder+\"/data_{}.csv\".format(str(key)), index_col=0)\n",
    "\n",
    "    return dictex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = ['tesla', 'facebook', 'microsoft', 'amazon', 'google', 'uber', 'lyft', 'apple', 'snap']\n",
    "key_terms = ['report', 'good', 'bad', 'up', 'down', 'stock']\n",
    "company_symbol = ['TSLA', 'FB', 'MSFT', 'AMZN', 'GOOGL', 'UBER', 'LYFT', 'AAPL', 'SNAP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create kw_list \n",
    "kw_list = []\n",
    "for c_name in companies:\n",
    "    for k in key_terms:\n",
    "        kw_list.append(c_name + \" \" + k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 3-month ago trend data. May need to run a couple times to make sure it doesn't time out. \n",
    "def getTrendDataDF(kw_list, pull_data=False, dates='today 3-m', pytrends_df_filename=\"pytrends.csv\"):\n",
    "    # Retrieve the cached trend data\n",
    "    if not pull_data and os.path.isfile(pytrends_df_filename):\n",
    "        df = pd.read_csv(pytrends_df_filename, index_col=\"date\")\n",
    "        \n",
    "    else: # Construct the dataframe by making pytrends calls\n",
    "        df = pd.DataFrame()\n",
    "        #print(df.empty)\n",
    "        pytrends = TrendReq(hl='en-US', tz=360)\n",
    "        for kw in kw_list:\n",
    "            pytrends.build_payload([kw], cat=0, timeframe=dates, geo='', gprop='')\n",
    "            df_temp = pytrends.interest_over_time()\n",
    "            df_temp = df_temp.drop(['isPartial'], axis=1)\n",
    "            # print(kw)\n",
    "            if df.empty:\n",
    "                df = df_temp\n",
    "            else:\n",
    "                df = df.join(df_temp)\n",
    "        df.to_csv(pytrends_df_filename)\n",
    "    #reverse df rows\n",
    "    df = df.iloc[::-1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Stock Daily info for past 100 days\n",
    "def getDailyStockInfoDict(company_symbol, pull_data=False, stock_df_filename=\"stock_keys.txt\"):\n",
    "    if not pull_data and os.path.isfile(stock_df_filename):\n",
    "        dict_stocks = load_dict_df(keys_filename=stock_df_filename);\n",
    "    else :\n",
    "        ts = 'TIME_SERIES_DAILY'\n",
    "        api_key = ''\n",
    "        outputsize = 'compact'\n",
    "        dict_stocks = {}\n",
    "        for i, symbol in enumerate(company_symbol):\n",
    "            link = 'https://www.alphavantage.co/query?function={}&symbol={}&apikey={}&outputsize={}'\\\n",
    "                    .format(ts, symbol, api_key, outputsize)\n",
    "            r = requests.get(link)\n",
    "            data = json.loads(r.text)\n",
    "            # API only give you 5 requests per 5 mins, so break out if response is not what is expected.\n",
    "            if \"Time Series (Daily)\" not in data:\n",
    "                break\n",
    "            stock_data_per_day = json.dumps(data[\"Time Series (Daily)\"])\n",
    "            df_temp = pd.read_json(stock_data_per_day).transpose()\n",
    "            df_temp.reset_index(level=0, inplace=True)\n",
    "            df_temp.columns = ['times', 'open', 'high', 'low', 'close', 'volume']\n",
    "            dict_stocks[companies[i]] = df_temp\n",
    "        save_dict_df(dict_stocks, keys_filename=stock_df_filename)\n",
    "    return dict_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrends(company_name, df_trends):\n",
    "    # Filters the trends\n",
    "    company_keywords =  [x for x in list(df_trends.columns.values) if company_name in x]\n",
    "    return df_trends[company_keywords].copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJointTimes(df_stock, df_trends): \n",
    "    stock_times = df_stock['times']\n",
    "    trends_times = list(df_trends.index)\n",
    "    return list(set(stock_times) & set(trends_times)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOverlapDateData(company_name, dict_stocks, df_trends): \n",
    "    new_df_trends = getTrends(company_name, df_trends)\n",
    "    \n",
    "    joint_times = getJointTimes(dict_stocks[company_name], new_df_trends)\n",
    "    \n",
    "    new_df_stocks = dict_stocks[company_name].loc[dict_stocks[company_name]['times'].isin(joint_times)]\n",
    "    new_df_stocks = new_df_stocks.reset_index() \n",
    "    \n",
    "    new_df_trends = new_df_trends.loc[new_df_trends.index.isin(joint_times)]\n",
    "    new_df_trends = new_df_trends.reset_index()\n",
    "    new_df_trends.columns = ['_'.join(x.split()) for x in list(new_df_trends.columns) if len(x) > 1]\n",
    "    df_all_data = new_df_stocks.join(new_df_trends)\n",
    "    return df_all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainMultipleRegression(y_col, parameter_cols, training_data): \n",
    "    formula = y_col + ' ~ 1'\n",
    "    for i in parameter_cols: \n",
    "        formula = formula + \" + \" + i\n",
    "\n",
    "    result = sm.formula.ols(formula=formula, data=training_data).fit()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMultipleRegressionAndGetMSE(result, y_col, cols, all_data, training_data, testing_data):\n",
    "    #y_pred = result.predict()\n",
    "    #     plt.plot(range(len(all_data[y_col])) , all_data[y_col])\n",
    "    #     plt.plot(range(50), result.predict(training_data[cols]), '-')\n",
    "    #     plt.plot(range(50, len(df_all_data[y_col])), result.predict(testing_data[cols]), '-')\n",
    "    #     plt.xlabel('Day')\n",
    "    #     plt.ylabel('Output $ amount')\n",
    "    #     plt.title('Multiple Regression: ' + str(y_col))\n",
    "    #     plt.legend([\"True Data\",\"Training Data - Price change prediction\", \"Testing Data - Price change prediction\"])\n",
    "    #     plt.show()\n",
    "    \n",
    "    MSE = mean_squared_error(result.predict(testing_data[cols]), testing_data[y_col])\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestAlphaRidgeRegression(y_col, parameter_cols, df_all_data_train):\n",
    "    df_all_data_test = df_all_data_train[40:]\n",
    "    df_all_data_train = df_all_data_train[:40]\n",
    "    \n",
    "    X = df_all_data_train[parameter_cols]\n",
    "    y = df_all_data_train[y_col]\n",
    "    alpha = []\n",
    "    MSE_train = []\n",
    "    MSE_test = []\n",
    "    for i in range(90, 10000, 10):\n",
    "        clf = Ridge(alpha=i)\n",
    "        clf.fit(X, y) \n",
    "        alpha.append(i)\n",
    "        MSE_train.append(mean_squared_error(clf.predict(df_all_data_train[cols]), df_all_data_train[y_col]))\n",
    "        MSE_test.append(mean_squared_error(clf.predict(df_all_data_test[cols]), df_all_data_test[y_col]))\n",
    "\n",
    "    bestAlpha = alpha[MSE_test.index(min(MSE_test))]\n",
    "    \n",
    "    print(\"alpha: \"+str(bestAlpha))\n",
    "    print(\"Training error = \"+str(mean_squared_error(clf.predict(df_all_data_train[cols]), df_all_data_train[y_col])))\n",
    "    print(\"Testing error = \"+str(mean_squared_error(clf.predict(df_all_data_test[cols]), df_all_data_test[y_col])))\n",
    "    print()\n",
    "    return bestAlpha, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ridge regression\n",
    "\n",
    "def trainAndPlotRidgeRegressionAndReturnMSE(y_col, parameter_cols, df_all_data_train, df_all_data_test) : \n",
    "    bestAlpha, clf = getBestAlphaRidgeRegression(y_col, parameter_cols, df_all_data_train)\n",
    "    \n",
    "    X = df_all_data_train[parameter_cols]\n",
    "    y = df_all_data_train[y_col]\n",
    "    \n",
    "    #plot data with training and test data\n",
    "    clf = Ridge(alpha=bestAlpha)\n",
    "    clf.fit(X, y) \n",
    "    #     plt.plot(range(len(df_all_data[y_col])) , df_all_data[y_col])\n",
    "    #     plt.plot(range(50), clf.predict(df_all_data_train[cols]), '-')\n",
    "    #     plt.plot(range(50, len(df_all_data[y_col])), clf.predict(df_all_data_test[cols]), '-')\n",
    "    #     plt.xlabel('Day')\n",
    "    #     plt.ylabel('Output $ amount')\n",
    "    #     plt.title('Ridge Regression: ' + str(y_col))\n",
    "    #     plt.legend([\"True Data\",\"Training Data - Price change prediction\", \"Testing Data - Price change prediction\"])\n",
    "    #     plt.show()\n",
    "    \n",
    "    MSE = mean_squared_error(clf.predict(df_all_data_test[cols]), df_all_data_test[y_col])\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMovingAvgAndStdDev(col_name, num_days, df_all_data):\n",
    "    # Moving average and stdev past X days\n",
    "    col_movingAvg = []\n",
    "    col_stdev = []\n",
    "    \n",
    "    num_days_to_average = 10\n",
    "    for i in range(len(df_all_data)):\n",
    "        sum_to_avg = 0\n",
    "        nums = []\n",
    "        num_to_avg = min(num_days_to_average, len(df_all_data) - i) - 1\n",
    "        for j in range(1, num_to_avg):\n",
    "            sum_to_avg += df_all_data[col][i + j]\n",
    "            nums.append(df_all_data[col][i + j])\n",
    "        avg = sum_to_avg / (num_to_avg if num_to_avg > 0 else 1)\n",
    "        stdev = np.std((nums if nums else [0]))\n",
    "\n",
    "        col_movingAvg.append(avg)\n",
    "        col_stdev.append(stdev)\n",
    "    return col_movingAvg, col_stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllParamCols(df_all_data):\n",
    "    cols = list(df_all_data.columns)\n",
    "    cols.remove('profit')\n",
    "    cols.remove('open')\n",
    "    cols.remove('close')\n",
    "    cols.remove('high')\n",
    "    cols.remove('low')\n",
    "    cols.remove('times')\n",
    "    cols.remove('date')\n",
    "    cols.remove('index')\n",
    "    cols.remove('volume')\n",
    "    cols.remove('mid')\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get trend data\n",
    "df_trends = getTrendDataDF(kw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stock data\n",
    "dict_stocks = getDailyStockInfoDict(company_symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with all predictors, and only moving averages, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MSE_dict = {} # {\"company\":{\"multipleRegressionProfit\":value, \"multipleRegressionCost\":value, etc}}\n",
    "# Join data together, train various models\n",
    "for company in companies[:5]: \n",
    "    print(company)\n",
    "    MSE_dict[company] = {}\n",
    "    # Get specific company data\n",
    "    df_all_data = getOverlapDateData(company, dict_stocks, df_trends)  \n",
    "    \n",
    "    \n",
    "    # Add more data columns\n",
    "    df_all_data['profit'] = df_all_data['open']-df_all_data['close']\n",
    "    df_all_data['mid'] = (df_all_data['high']+df_all_data['low'])/2\n",
    "    \n",
    "    # Trends cols (everything except these cols that are removed)\n",
    "    cols = getAllParamCols(df_all_data)\n",
    "    \n",
    "    # Include moving average, stdev, and prev. 10 days. \n",
    "    num_days_to_average = 10\n",
    "    col = 'profit'\n",
    "    col_movingAvg, col_stdev = getMovingAvgAndStdDev(col, num_days_to_average, df_all_data)\n",
    "    df_all_data[col+'_movingAvg'] = col_movingAvg\n",
    "    df_all_data[col+'_stdev'] = col_stdev\n",
    "    col_prev = [df_all_data[col][i+1] for i in range(len(df_all_data) - 1)]\n",
    "    col_prev.append(0) # Append this so we can have 0 padding\n",
    "    df_all_data[col+'_prev'] = col_prev\n",
    "    \n",
    "    col = 'mid'\n",
    "    col_movingAvg, col_stdev = getMovingAvgAndStdDev(col, num_days_to_average, df_all_data)\n",
    "    df_all_data[col+'_movingAvg'] = col_movingAvg\n",
    "    df_all_data[col+'_stdev'] = col_stdev\n",
    "    col_prev = [df_all_data[col][i+1] for i in range(len(df_all_data) - 1)]\n",
    "    col_prev.append(0) # Append this so we can have 0 padding\n",
    "    df_all_data[col+'_prev'] = col_prev\n",
    "    \n",
    "    col = 'volume'\n",
    "    col_movingAvg, col_stdev = getMovingAvgAndStdDev(col, num_days_to_average, df_all_data)\n",
    "    df_all_data[col+'_movingAvg'] = col_movingAvg\n",
    "    df_all_data[col+'_stdev'] = col_stdev\n",
    "    col_prev = [df_all_data[col][i+1] for i in range(len(df_all_data) - 1)]\n",
    "    col_prev.append(0) # Append this so we can have 0 padding\n",
    "    df_all_data[col+'_prev'] = col_prev\n",
    "    \n",
    "    # Include movingAvg, stdev, and prev for all the Trends cols\n",
    "    for col in cols: \n",
    "        # Prev info\n",
    "        col_prev = [df_all_data[col][i+1] for i in range(len(df_all_data) - 1)]\n",
    "        col_prev.append(0) # Append this so we can have 0 padding\n",
    "        df_all_data[col+'_prev'] = col_prev\n",
    "        \n",
    "        col_movingAvg, col_stdev = getMovingAvgAndStdDev(col, num_days_to_average, df_all_data)\n",
    "        \n",
    "        df_all_data[col+'_movingAvg'] = col_movingAvg\n",
    "        df_all_data[col+'_stdev'] = col_stdev\n",
    "\n",
    "    \n",
    "    cols = getAllParamCols(df_all_data)\n",
    "        \n",
    "    # Reverse index order\n",
    "    df_all_data = df_all_data.iloc[::-1] \n",
    "    \n",
    "    # Split into training and testing data\n",
    "    df_all_data_train = df_all_data[:50]\n",
    "    df_all_data_test = df_all_data[50:]\n",
    "    \n",
    "    MSE_dict[company][\"MultipleRegression\"] = {}\n",
    "    MSE_dict[company][\"RidgeRegression\"] = {}\n",
    "    parameter_type = \"allPredictors\"\n",
    "    MSE_dict[company][\"MultipleRegression\"][parameter_type] = {}\n",
    "    MSE_dict[company][\"RidgeRegression\"][parameter_type] = {}\n",
    "                         \n",
    "    # Multiple Regression, All Predictors\n",
    "    y_col='close'\n",
    "    close_multiple_regression_res = trainMultipleRegression(y_col, cols, df_all_data_train)\n",
    "    MSE = plotMultipleRegressionAndGetMSE(close_multiple_regression_res, y_col, cols, df_all_data, df_all_data_train, df_all_data_test)\n",
    "    MSE_dict[company][\"MultipleRegression\"][parameter_type][y_col] = MSE\n",
    "    print(\"MSE:\", MSE)\n",
    "    \n",
    "    \n",
    "    # RIDGE REGRESSION REQUIRES STANDARDIZATION\n",
    "    # Standardize the input parameters\n",
    "    df_standardized_data = df_all_data.copy()\n",
    "\n",
    "    for col in cols: \n",
    "        df_standardized_data[col] = df_standardized_data[col] / np.std(df_standardized_data[col].values)\n",
    "    \n",
    "    # Split into training and testing data\n",
    "    df_all_data_train = df_standardized_data[:50]\n",
    "    df_all_data_test = df_standardized_data[50:]\n",
    "    \n",
    "    # Ridge Regression, All Predictors\n",
    "    y_col='close'\n",
    "    MSE = trainAndPlotRidgeRegressionAndReturnMSE(y_col, cols, df_all_data_train, df_all_data_test)\n",
    "    MSE_dict[company][\"RidgeRegression\"][parameter_type][y_col] = MSE\n",
    "    print(\"MSE:\", MSE)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Predictions using only stock market data #### \n",
    "    # Remove any cols in col that has the company name in it. \n",
    "    print(\"No Trend Data\")\n",
    "    cols_noTrends = [col for col in cols if company not in col]\n",
    "    cols = cols_noTrends\n",
    "    \n",
    "    # Split into training and testing data\n",
    "    df_all_data_train = df_all_data[:50]\n",
    "    df_all_data_test = df_all_data[50:]\n",
    "    \n",
    "    parameter_type = \"noTrend\"\n",
    "    MSE_dict[company][\"MultipleRegression\"][parameter_type] = {}\n",
    "    MSE_dict[company][\"RidgeRegression\"][parameter_type] = {}\n",
    "    \n",
    "    # Multiple Regression\n",
    "    y_col='close'\n",
    "    close_multiple_regression_res = trainMultipleRegression(y_col, cols, df_all_data_train)\n",
    "    MSE = plotMultipleRegressionAndGetMSE(close_multiple_regression_res, y_col, cols, df_all_data, df_all_data_train, df_all_data_test)\n",
    "    MSE_dict[company][\"MultipleRegression\"][parameter_type][y_col] = MSE\n",
    "    print(\"MSE:\", MSE)\n",
    "    \n",
    "    # Standardize the input parameters\n",
    "    df_standardized_data = df_all_data.copy()\n",
    "\n",
    "    for col in cols: \n",
    "        df_standardized_data[col] = df_standardized_data[col] / np.std(df_standardized_data[col].values)\n",
    "    \n",
    "    # Split into training and testing data\n",
    "    df_all_data_train = df_standardized_data[:50]\n",
    "    df_all_data_test = df_standardized_data[50:]\n",
    "    \n",
    "    # Ridge Regression\n",
    "    y_col='close'\n",
    "    MSE = trainAndPlotRidgeRegressionAndReturnMSE(y_col, cols, df_all_data_train, df_all_data_test)\n",
    "    MSE_dict[company][\"RidgeRegression\"][parameter_type][y_col] = MSE\n",
    "    print(\"MSE:\", MSE)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with only Google Trends data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Join data together, train various models\n",
    "for company in companies[:5]: \n",
    "    print(company)\n",
    "    # Get specific company data\n",
    "    df_all_data = getOverlapDateData(company, dict_stocks, df_trends)  \n",
    "    \n",
    "    \n",
    "    # Add more data columns\n",
    "    df_all_data['profit'] = df_all_data['open']-df_all_data['close']\n",
    "    df_all_data['mid'] = (df_all_data['high']+df_all_data['low'])/2\n",
    "    \n",
    "    # Trends cols (everything except these cols that are removed)\n",
    "    cols = getAllParamCols(df_all_data)\n",
    "    \n",
    "    num_days_to_average = 10\n",
    "    for col in cols: \n",
    "        # Prev to today increment amount\n",
    "        col_prev = [df_all_data[col][i+1] for i in range(len(df_all_data) - 1)]\n",
    "        col_prev.append(0) # Append this so we can have 0 padding\n",
    "        df_all_data[col+'_prev'] = col_prev\n",
    "        \n",
    "        col_movingAvg, col_stdev = getMovingAvgAndStdDev(col, num_days_to_average, df_all_data)\n",
    "        \n",
    "        df_all_data[col+'_movingAvg'] = col_movingAvg\n",
    "        df_all_data[col+'_stdev'] = col_stdev\n",
    "\n",
    "    \n",
    "    cols = getAllParamCols(df_all_data)\n",
    "        \n",
    "    # Reverse index order\n",
    "    df_all_data = df_all_data.iloc[::-1] \n",
    "    \n",
    "    # Split into training and testing data\n",
    "    df_all_data_train = df_all_data[:50]\n",
    "    df_all_data_test = df_all_data[50:]\n",
    "    \n",
    "    \n",
    "    parameter_type=\"onlyTrends\"\n",
    "    MSE_dict[company][\"MultipleRegression\"][parameter_type] = {}\n",
    "    MSE_dict[company][\"RidgeRegression\"][parameter_type] = {}\n",
    "    \n",
    "    # Profit prediction: \n",
    "    y_col='close'\n",
    "    close_multiple_regression_res = trainMultipleRegression(y_col, cols, df_all_data_train)\n",
    "    MSE = plotMultipleRegressionAndGetMSE(close_multiple_regression_res, y_col, cols, df_all_data, df_all_data_train, df_all_data_test)\n",
    "    MSE_dict[company][\"MultipleRegression\"][parameter_type][y_col] = MSE\n",
    "    print(\"MSE:\", MSE)\n",
    "    \n",
    "    # Standardize the input parameters\n",
    "    df_standardized_data = df_all_data.copy()\n",
    "\n",
    "    for col in cols: \n",
    "        df_standardized_data[col] = df_standardized_data[col] / np.std(df_standardized_data[col].values)\n",
    "    \n",
    "    # Split into training and testing data\n",
    "    df_all_data_train = df_standardized_data[:50]\n",
    "    df_all_data_test = df_standardized_data[50:]\n",
    "    \n",
    "    \n",
    "    # Ridge Regression\n",
    "    y_col='close'\n",
    "    MSE = trainAndPlotRidgeRegressionAndReturnMSE(y_col, cols, df_all_data_train, df_all_data_test)\n",
    "    MSE_dict[company][\"RidgeRegression\"][parameter_type][y_col] = MSE\n",
    "    print(\"MSE:\", MSE)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMSE(y_col, model):\n",
    "    for company in companies[0:5]:\n",
    "        plt.plot([\"all predictors\", \"only trends\", \"no trends\"], [MSE_dict[company][model]['allPredictors'][y_col],  MSE_dict[company][model]['onlyTrends'][y_col], MSE_dict[company][model]['noTrend'][y_col]], label=company)\n",
    "\n",
    "    plt.xlabel(\"parameter types\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    plt.title(model + \" \" + y_col);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = \"high\"\n",
    "model = \"MultipleRegression\"\n",
    "plotMSE(y_col, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = \"mid\"\n",
    "model = \"MultipleRegression\"\n",
    "plotMSE(y_col, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = \"close\"\n",
    "model = \"MultipleRegression\"\n",
    "plotMSE(y_col, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = \"high\"\n",
    "model = \"RidgeRegression\"\n",
    "plotMSE(y_col, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = \"mid\"\n",
    "model = \"RidgeRegression\"\n",
    "plotMSE(y_col, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = \"close\"\n",
    "model = \"RidgeRegression\"\n",
    "plotMSE(y_col, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow + Keras time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data['mid'] = (df_all_data['high'] + df_all_data['low'])/2\n",
    "df_all_data[cols+['mid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data to be between 0-1 \n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "data_set_scaled = sc.fit_transform(df_standardized_data[cols+['mid']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test set\n",
    "train = data_set_scaled[:50, :]\n",
    "test = data_set_scaled[50:, :]\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential NN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_X, train_y, epochs=500, batch_size=72, validation_data=(test_X, test_y), verbose=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_y)+len(test_y)) , list(np.array(train_y))+list(np.array(test_y)), range(len(train_y)), model.predict(train_X), '-', range(len(train_y), len(train_y)+len(test_y)), model.predict(test_X), '-')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Profit from previous day')\n",
    "plt.title('Keras - Stock Market Mid per Day from 3 months until today')\n",
    "plt.legend([\"True Data\",\"Training Data - Price change prediction\", \"Testing Data - Price change prediction\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks['microsoft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msft = df_stocks['microsoft'][['high', 'low']].iloc[::-1]\n",
    "df_msft['mid'] = (df_msft['high'] + df_msft['low']) /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "data_set_scaled = sc.fit_transform(df_msft[['high', 'low', 'mid']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_set_scaled[:50, :]\n",
    "test = data_set_scaled[50:, :]\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_X, train_y, epochs=500, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(100) , list(np.array(train_y))+list(np.array(test_y)), range(50), model.predict(train_X), '-', range(50, 100), model.predict(test_X), '-')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Profit from previous day')\n",
    "plt.title('MSFT Keras- Stock Market Mid per Day from 3 months until today')\n",
    "plt.legend([\"True Data\",\"Training Data - Price change prediction\", \"Testing Data - Price change prediction\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msft_names = [x for x in list(df.columns.values) if 'microsoft' in x]\n",
    "df_msft_trends = df[msft_names]\n",
    "stock_times = df_stocks['microsoft'].times\n",
    "trends_times = list(df_msft_trends.index)\n",
    "joint_times = list(set(stock_times) & set(trends_times)) \n",
    "df_stocks['microsoft'] = df_stocks['microsoft'].loc[df_stocks['microsoft']['times'].isin(joint_times)]\n",
    "df_stocks['microsoft'] = df_stocks['microsoft'].reset_index()\n",
    "print(df_stocks['microsoft'].head())\n",
    "df_msft_trends = df_msft_trends.loc[df_msft_trends.index.isin(joint_times)]\n",
    "df_msft_trends = df_msft_trends.reset_index()\n",
    "df_msft_trends.columns = ['_'.join(x.split()) for x in list(df_msft_trends.columns) if len(x) > 1]\n",
    "print(df_msft_trends.head())\n",
    "df_msft = df_msft_trends.join(df_stocks['microsoft'])\n",
    "df_msft['profit'] = df_msft['open']-df_msft['close']\n",
    "df_msft = df_msft.iloc[::-1]\n",
    "df_msft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['microsoft_'+ x for x in key_terms]\n",
    "df_msft['mid'] = (df_fb['high'] + df_fb['low'])/2\n",
    "df_msft[cols+['high', 'low']+['mid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "data_set_scaled = sc.fit_transform(df_msft[cols+['high', 'low', 'mid']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_set_scaled[:50, :]\n",
    "test = data_set_scaled[50:, :]\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_X, train_y, epochs=500, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_y)+len(test_y)) , list(np.array(train_y))+list(np.array(test_y)), range(50), model.predict(train_X), '-', range(50, len(train_y)+len(test_y)), model.predict(test_X), '-')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Profit from previous day')\n",
    "plt.title('MSFT Keras- Stock Market Mid per Day from 3 months until today')\n",
    "plt.legend([\"True Data\",\"Training Data - Price change prediction\", \"Testing Data - Price change prediction\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
